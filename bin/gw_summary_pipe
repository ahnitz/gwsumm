#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) Duncan Macleod (2013)
#
# This file is part of GWSumm.
#
# GWSumm is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# GWSumm is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with GWSumm.  If not, see <http://www.gnu.org/licenses/>.

"""Pipeline generator for the Gravitational-wave interferometer
summary information system (`gw_summary`)

This module constructs a directed, acyclic graph (DAG) that defines
a workflow to be submitted via the HTCondor scheduler
"""

import argparse
import os
import shutil
from multiprocessing import cpu_count

from glue import (datafind, pipeline)

from gwpy.io.nds import kerberos as gwkerberos

from gwsumm import version
from gwsumm.utils import (mkdir, which)

__version__ = version.version
__author__ = 'Duncan Macleod <duncan.macleod@ligo.org>'


# ----------------------------------------------------------------------------
# Define gw_summary job

class GWSummaryJob(pipeline.CondorDAGJob):
    """Job representing a configurable instance of gw_summary.
    """
    logtag = '$(cluster)-$(process)'

    def __init__(self, universe, executable, tag='gw_summary',
                 subdir=None, logdir=None, **cmds):
        pipeline.CondorDAGJob.__init__(self, universe, executable)
        if subdir:
            subdir = os.path.abspath(subdir)
            self.set_sub_file(os.path.join(subdir, '%s.sub' % (tag)))
        if logdir:
            logdir = os.path.abspath(logdir)
            self.set_log_file(os.path.join(
                logdir, '%s-%s.log' % (tag, self.logtag)))
            self.set_stderr_file(os.path.join(
                logdir, '%s-%s.err' % (tag, self.logtag)))
            self.set_stdout_file(os.path.join(
                logdir, '%s-%s.out' % (tag, self.logtag)))
        cmds.setdefault('getenv', 'True')
        for key, val in cmds.iteritems():
            self.add_condor_cmd(key, val)

    def add_opt(self, opt, value=''):
        pipeline.CondorDAGJob.add_opt(self, opt, str(value))
    add_opt.__doc__ = pipeline.CondorDAGJob.add_opt.__doc__

# ----------------------------------------------------------------------------
# Parse command line

class GWHelpFormatter(argparse.HelpFormatter):
    def __init__(self, *args, **kwargs):
        kwargs.setdefault('indent_increment', 4)
        super(GWHelpFormatter, self).__init__(*args, **kwargs)

usage = ('%s --global-config defaults.ini --config-file myconfig.ini '
         '[--config-file myconfig2.ini] [options]'
         % os.path.basename(__file__))

parser = argparse.ArgumentParser(usage=usage, description=__doc__,
                                 formatter_class=GWHelpFormatter)

parser.add_argument("-p", "--profile", action="store_true", default=False,
                    help="show second timer with verbose statements, "
                         "default: %(default)s")
parser.add_argument("-v", "--verbose", action="store_true", default=False,
                    help="show verbose output, default: %(default)s")
parser.add_argument("-V", "--version", action="version",
                    help="show program's version number and exit")
parser.version = __version__

bopts = parser.add_argument_group("Basic options")
bopts.add_argument('-i', '--ifo', action='store', type=str, metavar='IFO',
                   help="Instrument to process. If this option is set in "
                        "the [DEFAULT] of any of the INI files, giving it "
                        "here is redundant.")
bopts.add_argument('-w', '--skip-html-wrapper', action='store_true',
                   default=False, help="Do not configure first job for HTML "
                                       "wrapper, default: %(default)s. Useful "
                                       "for separating large summary pipeline "
                                       "across multiple DAGs")
bopts.add_argument('-t', '--file-tag', action='store', type=str,
                   default=os.path.basename(__file__),
                   help="file tag for pipeline files, default: %(default)s")

htcopts = parser.add_argument_group("Condor options")
htcopts.add_argument('-x','--executable', action='store', type=str,
                     default=which('gw_summary'),
                     help="Path to gw_summary executable, default: %(default)s")
htcopts.add_argument('-u', '--universe', action='store', type=str,
                     default='vanilla',
                     help="Universe for condor jobs, default: %(default)s")
htcopts.add_argument('-l', '--log-dir', action='store', type=str,
                     default=os.environ.get('LOCALDIR', None),
                     help="Directory path for condor log files, "
                          "default: %(default)s")
htcopts.add_argument('-m', '--maxjobs', action='store', type=int, default=None,
                     metavar='N', help="Restrict the DAG to submit only N jobs "
                                       "at any one time, default: %(default)s")
htcopts.add_argument('-c', '--condor-command', action='append', type=str,
                     default=[],
                     help="Extra condor submit commands to add to "
                          "gw_summary submit file. Can be given "
                          "multiple times in the form \"key=value\"")

copts = parser.add_argument_group("Configuration options",
                                  "Each --global-config file will be used in "
                                  "all nodes of the workflow, while a single "
                                  "node will be created for each other "
                                  "--config-file")
copts.add_argument('-f', '--config-file', action='append', type=str,
                   metavar='FILE', default=[],
                   help="INI file for analysis, may be given multiple times")
copts.add_argument('-g', '--global-config', action='append', type=str,
                   metavar='FILE', default=[],
                   help="INI file for use in all workflow jobs, may be given "
                        "multiple times")

popts = parser.add_argument_group("Process options",
                                  "Configure how this summary will be "
                                  "processed.")
popts.add_argument('--nds', action='store_true', default='guess',
                   help='use NDS as the data source, default: %(default)s')
popts.add_argument('--single-process', action='store_true', default=False,
                   help="restrict gw_summary to a single process, mainly for "
                        "debugging purposes, default: %(default)s")
popts.add_argument('--max-processes', action='store', type=int, default=None,
                   help="maximum number of concurrent sub-processes for each "
                        "gw_summary job, {number of CPUs} / {min(number of "
                        "jobs, 4)}")
popts.add_argument('-a', '--archive', action='store_true', default=False,
                   help="Read archived data from the FILE, and "
                        "write back to it at the end")

outopts = parser.add_argument_group("Output options")
outopts.add_argument('-o', '--output-dir', action='store', type=str,
                     metavar='OUTDIR', default=os.curdir,
                     help="Output directory for summary information, "
                          "default: '%(default)s'")

topts = parser.add_argument_group("Time mode options",
                                  "Choose a stadard time mode, or a GPS "
                                  "[start, stop) interval")
topts.add_argument("--day", action="store", type=str, metavar='YYYYMMDD',
                   help="day to process.")
topts.add_argument("--week", action="store", type=str, metavar="YYYYMMDD",
                   help="week to process (by starting day).")
topts.add_argument("--month", action="store", type=str, metavar="YYYYMM",
                   help="month to process.")
topts.add_argument("--year", action="store", type=str, metavar="YYYY",
                   help="year to process.")
topts.add_argument("-s", "--gps-start-time", action="store", type=int,
                   metavar="GPSSTART", help="GPS start time")
topts.add_argument("-e", "--gps-end-time", action="store", type=int,
                   metavar="GPSEND", help="GPS end time")

opts = parser.parse_args()

# check time options
N = sum([opts.day is not None, opts.month is not None,
         opts.gps_start_time is not None, opts.gps_end_time is not None])
if N > 1 and not (opts.gps_start_time and opts.gps_end_time):
    raise parser.error("Please give only one of --day, --month, or "
                       "--gps-start-time and --gps-end-time.")

for i,cf in enumerate(opts.config_file):
    opts.config_file[i] = ','.join(map(os.path.abspath, cf.split(',')))
opts.global_config = map(os.path.abspath, [fp for csv in opts.global_config
                                           for fp in csv.split(',')])

# ----------------------------------------------------------------------------
# Build workflow directories

# move to output directory
mkdir(opts.output_dir)
os.chdir(opts.output_dir)
outdir = os.curdir

# set node log path, and condor log path
logdir = os.path.join(outdir, 'logs')
if opts.log_dir:
    htclogdir = opts.log_dir
else:
    htclogdir = logdir
mkdir(logdir, htclogdir)

# set config directory and copy config files
etcdir = os.path.join(outdir, 'etc')
mkdir(etcdir)

for i, fp in enumerate(opts.global_config):
    inicopy = os.path.join(etcdir, os.path.basename(fp))
    if not os.path.isfile(inicopy) or not os.path.samefile(fp, inicopy):
        shutil.copyfile(fp, inicopy)
    opts.global_config[i] = os.path.abspath(inicopy)
for i, csv in enumerate(opts.config_file):
    inicopy = []
    for fp in csv.split(','):
        fp2 = os.path.join(etcdir, os.path.basename(fp))
        if not os.path.isfile(fp2) or not os.path.samefile(fp, fp2):
            shutil.copyfile(fp, fp2)
        inicopy.append(os.path.abspath(fp2))
    opts.config_file[i] = ','.join(inicopy)
if opts.verbose:
    print("Copied all INI configuration files to %s." % etcdir)

# ----------------------------------------------------------------------------
# Configure X509 and kerberos for condor

if opts.universe != 'local':
    # copy X509 grid certificate into local location
    x509cert, x509key = datafind.find_credential()
    x509copy = os.path.join(etcdir, os.path.basename(x509cert))
    shutil.copyfile(x509cert, x509copy)

    # rerun kerberos with new path
    krb5cc = os.path.abspath(os.path.join(etcdir, 'krb5cc.krb5'))
    gwkerberos.kinit(krb5ccname=krb5cc)
    if opts.verbose:
        print("Configured Condor and Kerberos for NFS-shared credentials.")

# ----------------------------------------------------------------------------
# Build DAG

dag = pipeline.CondorDAG(os.path.join(htclogdir, '%s.log' % opts.file_tag))
dag.set_dag_file(os.path.join(outdir, opts.file_tag))

universe = opts.universe
executable = opts.executable

# ----------------------------------------------------------------------------
# Parse condor commands

# parse command line condor commands into dict
condorcmds = {}
for cmd_ in opts.condor_command:
    key, value = cmd_.split('=', 1)
    condorcmds[key.rstrip().lower()] = value.strip()

if opts.universe != 'local':
    # add X509 to environment
    for env_, val_ in zip(['X509_USER_PROXY', 'KRB5CCNAME'],
                          [os.path.abspath(x509copy), krb5cc]):
        condorenv = '%s=%s' % (env_, val_)
        if ('environment' in condorcmds and
            env_ not in condorcmds['environment']):
            condorcmds['environment'] += ';%s' % condorenv
        elif not 'environment' in condorcmds:
            condorcmds['environment'] = condorenv

# ----------------------------------------------------------------------------
# Build individual gw_summary jobs

job = GWSummaryJob(universe, executable, subdir=outdir, logdir=logdir,
                   tag=opts.file_tag, **condorcmds)
wrapjob = GWSummaryJob('local', executable, subdir=outdir, logdir=logdir,
                       tag='%s_local' % opts.file_tag)

# add global configurations
if len(opts.global_config):
    job.add_file_opt('config-file', ','.join(opts.global_config))

# add common command-line options
if opts.day:
    job.add_opt('day', opts.day)
elif opts.week:
    job.add_opt('week', opts.week)
elif opts.month:
    job.add_opt('month', opts.month)
elif opts.year:
    job.add_opt('year', opts.year)
elif opts.gps_start_time or opts.gps_end_time:
    job.add_opt('gps-start-time', str(opts.gps_start_time))
    job.add_opt('gps-end-time', str(opts.gps_end_time))
if opts.nds is True:
    job.add_opt('nds')
if opts.single_process:
    job.add_opt('single-process')
elif opts.max_processes is not None:
    job.add_opt('max-processes', opts.max_processes)
if opts.verbose:
    job.add_opt('verbose')
if opts.ifo:
    job.add_opt('ifo', opts.ifo)
job.add_opt('output-dir', outdir)

# make surrounding HTML first
if not opts.skip_html_wrapper:
    wrapper = pipeline.CondorDAGNode(wrapjob)
    wrapper.add_var_arg('--html-only')
    wrapper.add_var_arg('--config-file %s' % ','.join(opts.config_file))
    for configfile in opts.config_file:
        wrapper.add_input_file(opts.config_file)
    wrapper.set_category('gw_summary')
    dag.add_node(wrapper)
    if opts.verbose:
        print("    Configured HTML wrapper job.")

wrapjob.__dict__ = job.__dict__.copy()
wrapjob.set_sub_file('%s_local.sub' % os.path.splitext(job.get_sub_file())[0])
wrapjob.set_universe('local')
wrapjob._CondorJob__condor_cmds = job._CondorJob__condor_cmds.copy()
wrapjob._CondorJob__condor_cmds.pop('request_memory', None)

# create node for each config file
for configfile in opts.config_file:
    node = pipeline.CondorDAGNode(job)
    node.add_var_arg('--no-html')
    node.add_var_arg('--config-file %s' % configfile)
    if opts.archive:
        jobtag = os.path.splitext(os.path.basename(configfile))[0]
        archivetag = jobtag.upper().replace('-', '_')
        if opts.ifo and archivetag.startswith('%s_' % opts.ifo.upper()):
            archivetag = archivetag[3:]
        node.add_var_arg('--archive-tag %s' % archivetag)
    for cf in configfile.split(','):
        node.add_input_file(cf)
    node.set_category('gw_summary')
    node.set_retry(1)
    if not opts.skip_html_wrapper:
        node.add_parent(wrapper)
    dag.add_node(node)
    if opts.verbose:
        print("    Configured job for config %s." % configfile)

if opts.maxjobs:
    dag.add_maxjobs_category('gw_summary', opts.maxjobs)

# ----------------------------------------------------------------------------
# finish up

dag.write_sub_files()
dag.write_dag()
dag.write_script()
if opts.verbose:
    print("Setup complete. DAG written to:")
print(os.path.abspath(dag.get_dag_file()))
